{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-15T09:31:38.263313Z",
     "start_time": "2025-07-15T09:31:37.589750Z"
    }
   },
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from utils import clean\n",
    "from dotenv import load_dotenv"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:31:42.534110Z",
     "start_time": "2025-07-15T09:31:42.528280Z"
    }
   },
   "cell_type": "code",
   "source": "load_dotenv()",
   "id": "17b8828b6f6e9f38",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:31:45.506787Z",
     "start_time": "2025-07-15T09:31:45.502896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir = \"../../documents\"\n",
    "documents_path = os.listdir(dir)\n",
    "documents_path = [f\"{dir}/{file}\" for file in documents_path]"
   ],
   "id": "417949d5084d6f1f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:31:53.811634Z",
     "start_time": "2025-07-15T09:31:49.078639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "documents = []\n",
    "for file in documents_path:\n",
    "    loader = PyPDFLoader(file)\n",
    "    loaded_docs = loader.load()\n",
    "    \n",
    "    for doc in loaded_docs:\n",
    "        doc.page_content = clean(doc.page_content)\n",
    "    \n",
    "    documents.extend(loaded_docs)"
   ],
   "id": "21e55c19daa2de7f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:32:47.805984Z",
     "start_time": "2025-07-15T09:32:44.756836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = SentenceTransformersTokenTextSplitter(tokens_per_chunk=300, chunk_overlap=30)\n",
    "chunks = text_splitter.split_documents(documents)"
   ],
   "id": "414fef6a67e26d5b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:32:14.696462Z",
     "start_time": "2025-07-15T09:32:11.998649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# embeddings = embedding_model.embed_documents([chunk.page_content for chunk in chunks])"
   ],
   "id": "27e4c9975e63a432",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:33:04.712737Z",
     "start_time": "2025-07-15T09:32:52.508046Z"
    }
   },
   "cell_type": "code",
   "source": "vectorstore = FAISS.from_documents(documents=chunks, embedding=embedding_model)",
   "id": "f3d02e8f0d8f678b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:33:06.358696Z",
     "start_time": "2025-07-15T09:33:06.294025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever  = vectorstore.as_retriever()\n",
    "llm = ChatMistralAI(model=\"mistral-medium-latest\", temperature=0.8, max_retries=2)\n",
    "\n",
    "rag_pipeline = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)"
   ],
   "id": "2191aa1393badfd7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:33:20.678804Z",
     "start_time": "2025-07-15T09:33:14.349776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"how use nural network in nlp\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "920aca02a869a1e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Neural networks are widely used in Natural Language Processing (NLP) for various tasks. Here are some key ways they are applied, based on the provided context:\n",
      "\n",
      "1. **Language Modeling and Word Prediction**: Neural networks can be used as language models to predict the next word in a sequence. They also generate word embeddings, such as Word2Vec or GloVe, which are dense representations of words that capture semantic meanings and can be used in other NLP tasks.\n",
      "\n",
      "2. **Sentiment Analysis**: Neural networks can classify the sentiment of a text. Instead of using hand-built features, they can learn features from the data by representing words as embeddings. For example, a feedforward network can take word embeddings as input and use a hidden layer to represent non-linear interactions between features, improving the sentiment classifier's performance.\n",
      "\n",
      "3. **Sequence Modeling Tasks**: Recurrent Neural Networks (RNNs), including Long Short-Term Memory (LSTM) networks, are particularly useful for handling the sequential nature of language. They can be applied to tasks like:\n",
      "   - **Language Modeling**: Predicting the next word in a sequence.\n",
      "   - **Text Classification**: Tasks like sentiment analysis.\n",
      "   - **Part-of-Speech Tagging**: Assigning parts of speech to words in a text, which is crucial for understanding the structure of sentences.\n",
      "\n",
      "4. **Feature Learning**: One of the main advantages of neural networks in NLP is their ability to learn representations or features from the data. Early layers in the network learn basic features, which are then utilized by later layers for more complex understanding and prediction tasks.\n",
      "\n",
      "5. **Training and Optimization**: Neural networks are trained using optimization algorithms like gradient descent and backpropagation. Backpropagation is used to compute the gradients of the loss function, which helps in adjusting the weights of the network to minimize the error.\n",
      "\n",
      "In summary, neural networks in NLP are used for tasks ranging from simple classification to complex sequence modeling, leveraging their ability to learn and represent features from data effectively.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:33:35.407811Z",
     "start_time": "2025-07-15T09:33:33.684685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"who author this book\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "ffbdeaf4f8a2e171",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The book is authored by Daniel Jurafsky and James H. Martin, as indicated by the line: \"speech and language processing. daniel jurafsky & james h. martin.\"\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:33:49.470726Z",
     "start_time": "2025-07-15T09:33:44.341790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"this book explain the lstm and rnn\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "33c132233e14cce9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Yes, this book explains both Long Short-Term Memory (LSTM) networks and Recurrent Neural Networks (RNNs).\n",
      "\n",
      "For RNNs, it covers the basic structure, how they process sequences one element at a time, and how the output of each neural unit at time \\( t \\) is based on both the current input and the hidden layer from the previous time step \\( t-1 \\). It also mentions that RNNs can be trained using backpropagation through time (BPTT) and discusses their limitations, such as the vanishing gradients problem.\n",
      "\n",
      "For LSTMs, the book describes them as an extension to RNNs designed to address the vanishing gradients problem. LSTMs use specialized neural units with gates to control the flow of information into and out of the units. These gates help the network learn to forget information that is no longer needed and to remember information required for future decisions. The book also explains that LSTMs have become the standard unit for modern systems using recurrent networks.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:34:05.761333Z",
     "start_time": "2025-07-15T09:33:56.091643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"who best naive bays or transformer\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "37ab015ac908bd0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Based on the provided context, it is not explicitly stated which model is better between Naive Bayes and Transformers. However, the context does mention some advantages and disadvantages of Naive Bayes and briefly touches on Transformers.\n",
      "\n",
      "Naive Bayes is noted for its simplicity, ease of implementation, and fast training times. It can work well on very small datasets or short documents and often makes correct classification decisions despite its conditional independence assumptions. However, it has limitations with larger documents or datasets and can be less accurate with correlated features.\n",
      "\n",
      "Transformers, on the other hand, are described in terms of their architecture and modularity, with a focus on dimensionality and attention mechanisms. They are generally more complex and powerful, suitable for a wide range of tasks, especially those involving sequential data like natural language processing.\n",
      "\n",
      "In summary, the choice between Naive Bayes and Transformers depends on the specific use case, dataset size, and computational resources. Naive Bayes might be preferable for smaller datasets or simpler tasks, while Transformers are better suited for more complex and larger-scale applications.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:34:19.458484Z",
     "start_time": "2025-07-15T09:34:18.250583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"In this book what the chapter number for  vector semantic\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "7139c4ce82c0388a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The chapter number for vector semantics in the provided context is **Chapter 6.2**. This section discusses vector semantics, including the representation of word meanings as points in a multi-dimensional space.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:34:39.643166Z",
     "start_time": "2025-07-15T09:34:33.835584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"what the transformer use case\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "8b071d8c5ab12f0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The context provided describes the architecture and components of a transformer, particularly in the context of language modeling. Based on this information, one of the primary use cases of transformers is for language modeling tasks. Here are some key points about transformer use cases from the context:\n",
      "\n",
      "1. **Language Modeling**: Transformers are used to build language models that predict the next token in a sequence. This involves encoding input tokens, passing them through stacked transformer blocks, and using a language model head to generate logits and word probabilities.\n",
      "\n",
      "2. **Self-Attention Mechanisms**: Transformers utilize multi-head attention, a form of self-attention, which allows the model to weigh the relevance of different tokens in the input sequence. This helps in capturing complex dependencies and relationships within the data.\n",
      "\n",
      "3. **Wide Context Window**: Transformer-based language models can handle large context windows, sometimes up to 200,000 tokens or more, enabling them to draw on extensive contextual information.\n",
      "\n",
      "4. **Versatility**: While the context primarily discusses language modeling, transformers are also widely used in various other natural language processing (NLP) tasks such as translation, text summarization, question answering, and more. They are also applied in other domains like computer vision and speech recognition.\n",
      "\n",
      "In summary, the primary use case of transformers, as described in the context, is language modeling, leveraging their ability to handle sequential data and capture long-range dependencies through self-attention mechanisms.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:35:26.567782Z",
     "start_time": "2025-07-15T09:35:22.013864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"what the naive bays use case\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "68fa742aff4101ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The context provided highlights several use cases and characteristics of the Naive Bayes classifier, particularly the multinomial Naive Bayes. Here are the key use cases mentioned:\n",
      "\n",
      "1. **Text Classification**: Naive Bayes is commonly used for text classification tasks, such as sentiment analysis, where it classifies text as reflecting positive or negative sentiment.\n",
      "\n",
      "2. **Small Datasets**: Naive Bayes can work extremely well on very small datasets, sometimes even outperforming logistic regression in such scenarios.\n",
      "\n",
      "3. **Short Documents**: It is effective for short documents, making it suitable for tasks involving brief text inputs.\n",
      "\n",
      "4. **Speed and Simplicity**: Naive Bayes is easy to implement and very fast to train, as it does not require an optimization step. This makes it a good choice for situations where computational resources are limited.\n",
      "\n",
      "5. **Binarized Features**: Naive Bayes with binarized features (where features are represented as binary values) often works better for many text classification tasks.\n",
      "\n",
      "Despite its simplifying assumptions, such as the bag-of-words assumption (where word position is ignored) and conditional independence (where words are assumed to be independent given the class), Naive Bayes can still make correct classification decisions in many practical scenarios.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:35:32.747963Z",
     "start_time": "2025-07-15T09:35:26.639240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"what the mask in transformer\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "508581e7e8e998c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "In the context of transformers, particularly in the Masked Language Model (MLM) training objective, the term \"mask\" refers to the process of randomly selecting some tokens in the input sequence and replacing them with a special [MASK] token or a random token. This masking technique is used to create a training task where the model must predict the original identities of the masked tokens.\n",
      "\n",
      "Here’s a breakdown of the masking process as described in the context:\n",
      "\n",
      "1. **Token Selection**: A subset of the input tokens is randomly selected for masking. For example, in the provided context, the tokens \"long,\" \"thanks,\" and \"the\" were sampled from the input sequence.\n",
      "\n",
      "2. **Masking**: Some of the selected tokens are replaced with a [MASK] token, while others might be replaced with a random token from the vocabulary. In the example, \"long\" and \"thanks\" were masked, and \"the\" was replaced with the unrelated word \"apricot.\"\n",
      "\n",
      "3. **Training Objective**: The model is then trained to predict the original tokens that were masked or replaced. The probabilities assigned by the model to these masked tokens are used to compute the training loss, typically using cross-entropy loss.\n",
      "\n",
      "The purpose of this masking technique is to enable the model to learn bidirectional contextual representations of words, as it must use the surrounding context to predict the masked words. This is a key aspect of models like BERT (Bidirectional Encoder Representations from Transformers).\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
