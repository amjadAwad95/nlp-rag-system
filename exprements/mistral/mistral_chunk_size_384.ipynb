{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-15T09:35:52.459874Z",
     "start_time": "2025-07-15T09:35:51.739841Z"
    }
   },
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from utils import clean\n",
    "from dotenv import load_dotenv"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:35:52.487711Z",
     "start_time": "2025-07-15T09:35:52.481837Z"
    }
   },
   "cell_type": "code",
   "source": "load_dotenv()",
   "id": "17b8828b6f6e9f38",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:35:54.225948Z",
     "start_time": "2025-07-15T09:35:54.221948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir = \"../../documents\"\n",
    "documents_path = os.listdir(dir)\n",
    "documents_path = [f\"{dir}/{file}\" for file in documents_path]"
   ],
   "id": "417949d5084d6f1f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:36:01.170223Z",
     "start_time": "2025-07-15T09:35:56.322035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "documents = []\n",
    "for file in documents_path:\n",
    "    loader = PyPDFLoader(file)\n",
    "    loaded_docs = loader.load()\n",
    "    \n",
    "    for doc in loaded_docs:\n",
    "        doc.page_content = clean(doc.page_content)\n",
    "    \n",
    "    documents.extend(loaded_docs)"
   ],
   "id": "21e55c19daa2de7f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text_splitter = SentenceTransformersTokenTextSplitter(tokens_per_chunk=384, chunk_overlap=40)\n",
    "chunks = text_splitter.split_documents(documents)"
   ],
   "id": "414fef6a67e26d5b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:36:20.426072Z",
     "start_time": "2025-07-15T09:36:14.127164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# embeddings = embedding_model.embed_documents([chunk.page_content for chunk in chunks])"
   ],
   "id": "27e4c9975e63a432",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:36:29.898739Z",
     "start_time": "2025-07-15T09:36:20.438338Z"
    }
   },
   "cell_type": "code",
   "source": "vectorstore = FAISS.from_documents(documents=chunks, embedding=embedding_model)",
   "id": "f3d02e8f0d8f678b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:36:32.400135Z",
     "start_time": "2025-07-15T09:36:32.332780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever  = vectorstore.as_retriever()\n",
    "llm = ChatMistralAI(model=\"mistral-medium-latest\", temperature=0.8, max_retries=2)\n",
    "\n",
    "rag_pipeline = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)"
   ],
   "id": "2191aa1393badfd7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:36:40.414916Z",
     "start_time": "2025-07-15T09:36:34.534530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"how use nural network in nlp\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "920aca02a869a1e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Based on the provided context, neural networks can be used in Natural Language Processing (NLP) in several ways:\n",
      "\n",
      "1. **Recurrent Neural Networks (RNNs)**: These networks are particularly useful for tasks involving sequential data, such as language modeling, text classification, and sequence modeling tasks like part-of-speech tagging. RNNs contain cycles within their network connections, allowing them to use their own earlier outputs as inputs. Elman networks and Long Short-Term Memory (LSTM) networks are examples of RNN architectures used in NLP.\n",
      "\n",
      "2. **Feedforward Networks**: These can be applied to classification tasks like sentiment analysis. Instead of using hand-built features, neural networks in NLP often use word embeddings (like word2vec or GloVe) to learn features from the data. For example, you can represent an input text by pooling the embeddings of all the words in the text, such as summing the embeddings or taking their mean.\n",
      "\n",
      "3. **Directional RNN Architectures**: RNNs can be combined in various ways to create complex networks. For instance, stacked RNNs can be used where the outputs of one RNN layer are used as inputs to another RNN layer, allowing for more complex representations of the input data.\n",
      "\n",
      "Here is a brief overview of how to use these networks in NLP tasks:\n",
      "\n",
      "- **Language Modeling**: RNNs can be used to predict the next word in a sequence, making them useful for tasks like text generation.\n",
      "- **Text Classification**: Feedforward networks can classify text into different categories, such as positive or negative sentiment in sentiment analysis.\n",
      "- **Sequence Modeling**: RNNs can be used for tasks like part-of-speech tagging, where each word in a sentence is tagged with its part of speech.\n",
      "\n",
      "The context also mentions that these networks can be treated as modules and combined in creative ways to build more complex architectures for various NLP tasks.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:36:42.058825Z",
     "start_time": "2025-07-15T09:36:40.445675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"who author this book\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "ffbdeaf4f8a2e171",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The context provided does not specify the author of a particular book. It mentions various authors and their works, such as Mosteller and Wallace's work on authorship attribution, but it does not provide information about the author of a specific book. If you have a particular book in mind, please provide more details.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:36:46.547427Z",
     "start_time": "2025-07-15T09:36:42.075314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"this book explain the lstm and rnn\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "33c132233e14cce9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Yes, the provided context explains both Long Short-Term Memory (LSTM) networks and Recurrent Neural Networks (RNNs).\n",
      "\n",
      "### RNN (Recurrent Neural Network):\n",
      "- **Structure**: RNNs have a recurrent connection in the hidden layer, which allows them to maintain a form of memory or context from previous time steps. This context can include information from the beginning of the sequence, making RNNs suitable for sequential data.\n",
      "- **Function**: At each time step, an input vector is processed, and the hidden layer's activation depends on both the current input and the previous hidden layer's activation. This recurrent link helps the network to remember past information.\n",
      "- **Application**: RNNs are used for tasks like sequence labeling, sequence classification, and language modeling.\n",
      "\n",
      "### LSTM (Long Short-Term Memory):\n",
      "- **Structure**: LSTMs are a more complex type of RNN designed to address the vanishing gradients problem. They include an explicit context layer and specialized neural units with gates to control the flow of information.\n",
      "- **Function**: LSTMs manage context by learning to forget information that is no longer needed and to remember information that is required for future decisions. They use gates implemented with sigmoid activation functions and pointwise multiplication to regulate the flow of information.\n",
      "- **Application**: LSTMs have become the standard unit for modern systems using recurrent networks due to their ability to maintain relevant context over time.\n",
      "\n",
      "The context also mentions different architectures for various NLP tasks, such as sequence labeling, sequence classification, and language modeling, and introduces the encoder-decoder model as another architecture used with RNNs.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:36:50.765298Z",
     "start_time": "2025-07-15T09:36:46.568887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"who best naive bays or transformer\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "37ab015ac908bd0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The choice between Naive Bayes and Transformer models depends on the specific task and context:\n",
      "\n",
      "1. **Naive Bayes**:\n",
      "   - **Pros**: Simple to implement, fast to train, and can work well with small datasets or short documents. It is a generative classifier that models how a class could generate input data.\n",
      "   - **Cons**: Less accurate with larger datasets or documents due to its \"naive\" assumption of feature independence. It can overestimate the evidence when features are correlated.\n",
      "\n",
      "2. **Transformers**:\n",
      "   - **Pros**: Highly accurate and modular, capable of capturing complex patterns and relationships in data. They use multi-head attention mechanisms to focus on different parts of the input for different purposes, making them very powerful for tasks like language understanding and generation.\n",
      "   - **Cons**: More complex and computationally intensive to train and deploy. They require large amounts of data and computational resources.\n",
      "\n",
      "In summary, if you have a small dataset or need a quick, simple solution, Naive Bayes might be sufficient. However, for more complex tasks and larger datasets, Transformer models generally provide superior performance.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:36:54.192529Z",
     "start_time": "2025-07-15T09:36:50.863101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"In this book what the chapter number for  vector semantic\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "7139c4ce82c0388a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The information about vector semantics is found in Chapter 6. This chapter discusses vector semantics, including the representation of words as vectors in high-dimensional space, also known as embeddings. It covers both sparse and dense vector semantic models and provides examples such as term-document matrices and word-context matrices.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:36:59.979589Z",
     "start_time": "2025-07-15T09:36:54.199553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"what the transformer use case\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "8b071d8c5ab12f0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The transformer architecture is primarily used for tasks in natural language processing (NLP) and has a wide range of applications due to its ability to handle sequential data and long-range dependencies effectively. Some of the key use cases include:\n",
      "\n",
      "1. **Machine Translation**: Transformers were initially introduced for machine translation tasks, where they have shown significant improvements over previous models like RNNs and CNNs.\n",
      "\n",
      "2. **Language Modeling**: Transformers are used to build language models that predict the next word in a sequence. These models can generate coherent and contextually relevant text over long distances.\n",
      "\n",
      "3. **Text Generation**: Beyond simple language modeling, transformers can generate creative and contextually appropriate text, making them useful for applications like chatbots, story generation, and more.\n",
      "\n",
      "4. **Text Summarization**: Transformers can be used to summarize long documents by identifying and extracting the most important information.\n",
      "\n",
      "5. **Sentiment Analysis**: They can analyze the sentiment of a given text, determining whether the expressed opinion is positive, negative, or neutral.\n",
      "\n",
      "6. **Named Entity Recognition (NER)**: Transformers can identify and classify named entities in text, such as names of people, organizations, locations, and more.\n",
      "\n",
      "7. **Question Answering**: They can be used to build systems that answer questions based on a given context or knowledge base.\n",
      "\n",
      "8. **Speech Recognition**: Transformers are also applied in automatic speech recognition (ASR) systems to convert spoken language into text.\n",
      "\n",
      "9. **Text Classification**: They can classify text into different categories, such as spam detection, topic classification, and more.\n",
      "\n",
      "The modularity and scalability of transformers make them highly versatile for various NLP tasks, and their ability to handle long-range dependencies and parallel processing makes them particularly powerful.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:37:05.796224Z",
     "start_time": "2025-07-15T09:36:59.987777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"what the naive bays use case\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "68fa742aff4101ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Naive Bayes classifiers are particularly useful in certain scenarios due to their simplicity, efficiency, and effectiveness. Here are some key use cases for Naive Bayes:\n",
      "\n",
      "1. **Text Classification**: Naive Bayes is widely used for text classification tasks, such as spam filtering (Metsis et al., 2006). It can efficiently handle the high-dimensional nature of text data and perform well even with small datasets or short documents (Wang and Manning, 2012).\n",
      "\n",
      "2. **Sentiment Analysis**: Naive Bayes can be used for sentiment analysis, where the goal is to classify text into categories like positive, negative, or neutral. Its simplicity and speed make it a good choice for such tasks.\n",
      "\n",
      "3. **Small Datasets**: Naive Bayes often performs well on very small datasets, sometimes even better than more complex models like logistic regression (Ng and Jordan, 2002). This makes it a suitable choice when the amount of training data is limited.\n",
      "\n",
      "4. **Fast Training**: Naive Bayes is easy to implement and very fast to train because it does not require an optimization step. This makes it a practical choice for applications where training time is a critical factor.\n",
      "\n",
      "5. **Baseline Model**: Due to its simplicity and speed, Naive Bayes is often used as a baseline model in classification tasks. It provides a quick and easy way to establish a performance benchmark that more complex models can be compared against.\n",
      "\n",
      "6. **Authorship Attribution**: Naive Bayes has been used in authorship attribution problems, such as determining the authorship of disputed texts (Mosteller and Wallace, 1963, 1964).\n",
      "\n",
      "Despite its simplifying assumptions and potential issues with correlated features, Naive Bayes remains a valuable tool in the machine learning toolkit, especially for the use cases mentioned above.\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
