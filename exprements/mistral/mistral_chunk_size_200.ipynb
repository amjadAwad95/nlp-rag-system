{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-15T09:27:45.238445Z",
     "start_time": "2025-07-15T09:27:44.554853Z"
    }
   },
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from utils import clean\n",
    "from dotenv import load_dotenv"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:27:46.838955Z",
     "start_time": "2025-07-15T09:27:46.831832Z"
    }
   },
   "cell_type": "code",
   "source": "load_dotenv()",
   "id": "17b8828b6f6e9f38",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:27:48.743292Z",
     "start_time": "2025-07-15T09:27:48.739357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir = \"../../documents\"\n",
    "documents_path = os.listdir(dir)\n",
    "documents_path = [f\"{dir}/{file}\" for file in documents_path]"
   ],
   "id": "417949d5084d6f1f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:27:55.933514Z",
     "start_time": "2025-07-15T09:27:51.140907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "documents = []\n",
    "for file in documents_path:\n",
    "    loader = PyPDFLoader(file)\n",
    "    loaded_docs = loader.load()\n",
    "    \n",
    "    for doc in loaded_docs:\n",
    "        doc.page_content = clean(doc.page_content)\n",
    "    \n",
    "    documents.extend(loaded_docs)"
   ],
   "id": "21e55c19daa2de7f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text_splitter = SentenceTransformersTokenTextSplitter(tokens_per_chunk=200, chunk_overlap=20)\n",
    "chunks = text_splitter.split_documents(documents)"
   ],
   "id": "414fef6a67e26d5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:28:15.834822Z",
     "start_time": "2025-07-15T09:28:13.181766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# embeddings = embedding_model.embed_documents([chunk.page_content for chunk in chunks])"
   ],
   "id": "27e4c9975e63a432",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:28:38.427449Z",
     "start_time": "2025-07-15T09:28:24.354684Z"
    }
   },
   "cell_type": "code",
   "source": "vectorstore = FAISS.from_documents(documents=chunks, embedding=embedding_model)",
   "id": "f3d02e8f0d8f678b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:28:49.337719Z",
     "start_time": "2025-07-15T09:28:49.271205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever  = vectorstore.as_retriever()\n",
    "llm = ChatMistralAI(model=\"mistral-medium-latest\", temperature=0.8, max_retries=2)\n",
    "\n",
    "rag_pipeline = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)"
   ],
   "id": "2191aa1393badfd7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:28:59.246704Z",
     "start_time": "2025-07-15T09:28:51.244752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"how use nural network in nlp\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "920aca02a869a1e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Based on the provided context, here are some ways neural networks are used in Natural Language Processing (NLP):\n",
      "\n",
      "1. **Feedforward Networks for Classification**:\n",
      "   - Tasks like sentiment analysis can be addressed using feedforward networks. Instead of using hand-built features, neural networks can learn features from the data by representing words as embeddings (e.g., word2vec or GloVe embeddings).\n",
      "   - One approach is to apply a pooling function to the embeddings of all the words in the input text to create a representation for classification.\n",
      "\n",
      "2. **Recurrent Neural Networks (RNNs)**:\n",
      "   - RNNs are used for tasks that involve sequential data. They contain cycles within their network connections, allowing the value of a unit to depend on its own earlier outputs.\n",
      "   - Elman networks, a type of RNN, are particularly useful for language tasks. They can be used for language modeling, text classification, and sequence modeling tasks like part-of-speech tagging.\n",
      "\n",
      "3. **Stacked RNNs**:\n",
      "   - RNNs can be stacked, meaning the outputs of one RNN can be used as inputs to another. This allows for more complex representations and can improve performance on various NLP tasks.\n",
      "\n",
      "4. **Neural Language Modeling**:\n",
      "   - Neural networks can be used to build language models that predict the probability of a sequence of words. This is useful for tasks like text generation and machine translation.\n",
      "\n",
      "5. **Sequence Modeling**:\n",
      "   - Tasks like part-of-speech tagging involve predicting a sequence of labels for a sequence of words. RNNs are well-suited for such tasks because they can capture dependencies between words in the sequence.\n",
      "\n",
      "In summary, neural networks in NLP are used for a variety of tasks including classification, language modeling, and sequence modeling, often leveraging embeddings and RNN architectures to learn from and make predictions on textual data.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:29:48.931133Z",
     "start_time": "2025-07-15T09:29:45.953440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \" author this book\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "ffbdeaf4f8a2e171",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The book \"Computer Power and Human Reason: From Judgement to Calculation\" was authored by Joseph Weizenbaum.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:30:02.186838Z",
     "start_time": "2025-07-15T09:29:52.812163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"this book explain the lstm and rnn\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "33c132233e14cce9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Yes, the provided context explains both Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks. Here's a summary of the explanations:\n",
      "\n",
      "### RNNs (Recurrent Neural Networks):\n",
      "- **Structure**: RNNs process sequences one element at a time. The output at each time step depends on the current input and the hidden layer from the previous time step.\n",
      "- **Training**: RNNs can be trained using an extension of the backpropagation algorithm known as Backpropagation Through Time (BPTT).\n",
      "- **Limitations**: Simple RNNs struggle with long sequences due to issues like vanishing gradients.\n",
      "\n",
      "### LSTMs (Long Short-Term Memory Networks):\n",
      "- **Structure**: LSTMs are a type of RNN with a more complex architecture that includes gating mechanisms. These gates decide what information to remember and what to forget, which helps in handling long-term dependencies.\n",
      "- **Usage**: LSTMs have become the standard unit for modern systems that use recurrent networks due to their ability to overcome the limitations of simple RNNs.\n",
      "\n",
      "The context also mentions various applications of RNNs, such as probabilistic language modeling, auto-regressive generation, and sequence labeling.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:30:21.652603Z",
     "start_time": "2025-07-15T09:30:14.552337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"who best naive bays or transformer\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "37ab015ac908bd0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The choice between Naive Bayes and Transformers depends on the specific use case and requirements of the task at hand.\n",
      "\n",
      "**Naive Bayes** has several advantages:\n",
      "- It is simple to implement and very fast to train.\n",
      "- It can work well on very small datasets or short documents.\n",
      "- It is a reasonable approach for tasks where the conditional independence assumptions are not severely violated.\n",
      "\n",
      "However, Naive Bayes has some limitations:\n",
      "- It makes strong conditional independence assumptions, which can lead to overestimating the evidence when features are correlated.\n",
      "- It is generally less accurate than discriminative classifiers like logistic regression, especially on larger datasets or documents.\n",
      "\n",
      "**Transformers**, on the other hand, are a more modern and complex architecture:\n",
      "- They are highly effective for tasks involving sequential data, such as natural language processing.\n",
      "- They can capture long-range dependencies and complex patterns in data due to their multi-head attention mechanisms.\n",
      "- They are generally more accurate and powerful but require more computational resources and data to train effectively.\n",
      "\n",
      "In summary:\n",
      "- If you have a small dataset or need a simple, fast-to-train model, Naive Bayes might be a good choice.\n",
      "- If you are working with large datasets and complex patterns, especially in tasks like language understanding or generation, Transformers are likely to perform better.\n",
      "\n",
      "Ultimately, the \"best\" model depends on the specific context, data size, computational resources, and the nature of the task.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:30:32.566033Z",
     "start_time": "2025-07-15T09:30:31.327053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"In this book what the chapter number for  vector semantic\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "7139c4ce82c0388a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The chapter number for vector semantics is Chapter 6. The section specifically discussing vector semantics is labeled as \"6.2 â€¢ vector semantics.\"\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:30:41.025588Z",
     "start_time": "2025-07-15T09:30:37.054440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"what the transformer use case\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "8b071d8c5ab12f0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The transformer architecture is primarily used for tasks in natural language processing (NLP) and has a wide range of applications. Some of the key use cases include:\n",
      "\n",
      "1. **Language Modeling**: Transformers are used to predict the next token in a sequence, which is fundamental for tasks like text generation.\n",
      "\n",
      "2. **Machine Translation**: They are highly effective in translating text from one language to another due to their ability to handle long-range dependencies and contextual information.\n",
      "\n",
      "3. **Text Summarization**: Transformers can generate concise and coherent summaries of longer texts by understanding and condensing the important information.\n",
      "\n",
      "4. **Sentiment Analysis**: They are used to analyze and determine the sentiment expressed in a piece of text, such as positive, negative, or neutral.\n",
      "\n",
      "5. **Question Answering**: Transformers can understand the context of a question and provide accurate answers based on a given passage or document.\n",
      "\n",
      "6. **Named Entity Recognition (NER)**: They identify and classify named entities in text, such as names of people, organizations, locations, etc.\n",
      "\n",
      "7. **Text Classification**: Transformers can classify text into different categories, which is useful for tasks like spam detection, topic labeling, and intent recognition.\n",
      "\n",
      "The transformer's ability to handle self-attention mechanisms allows it to capture complex relationships and dependencies within the data, making it highly versatile for various NLP tasks.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T09:31:13.685540Z",
     "start_time": "2025-07-15T09:31:06.897458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"what the naive bays use case\"\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response[\"result\"])"
   ],
   "id": "68fa742aff4101ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "Naive Bayes classifiers have several use cases where they perform well despite their simplifying assumptions. Here are some key scenarios where Naive Bayes is particularly useful:\n",
      "\n",
      "1. **Small Datasets**: Naive Bayes can work extremely well on very small datasets, sometimes even outperforming more complex models like logistic regression.\n",
      "\n",
      "2. **Short Documents**: For text classification tasks involving short documents, Naive Bayes can be very effective.\n",
      "\n",
      "3. **Speed and Simplicity**: Naive Bayes is easy to implement and very fast to train because it does not require an optimization step. This makes it a good choice for applications where training time and computational resources are limited.\n",
      "\n",
      "4. **Baseline Model**: Due to its simplicity and speed, Naive Bayes is often used as a baseline model to compare against more complex algorithms.\n",
      "\n",
      "5. **Text Classification**: Naive Bayes is commonly used in text classification tasks, such as spam filtering, sentiment analysis, and topic classification. The multinomial variant is particularly suited for these tasks as it models the frequency of words in documents.\n",
      "\n",
      "6. **Initial Prototyping**: Because of its straightforward implementation and quick training time, Naive Bayes is useful for initial prototyping and testing of classification tasks.\n",
      "\n",
      "Despite its conditional independence assumptions, which can be overly strong and lead to overestimation of evidence when features are correlated, Naive Bayes often makes correct classification decisions and remains a reasonable approach for many applications.\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
